{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9efe796-6ba5-4572-9ab9-ecb56ff73aee",
   "metadata": {},
   "source": [
    "# COVID-19 Data EDA\n",
    "\n",
    "*This document performs EDA on a dataset containing data from COVID-19 event.*\n",
    "\n",
    "- **Author:** Sakthi Santhosh\n",
    "- **Created on:** 23/08/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b4c0a3-931c-4c0c-9429-be89424b6fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attack Helicopter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6e4d7-80c9-4810-81a7-387159d233f7",
   "metadata": {},
   "source": [
    "## Creating a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716264a8-eb2e-4ad9-b490-7dc3a85a5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calendar import month_name\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, regexp_replace, date_format,\n",
    "    to_date, when, lower, month\n",
    ")\n",
    "from pyspark.sql.functions import sum as pyspark_sum\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, FloatType,\n",
    "    IntegerType, StringType\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"covid-data-eda\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1612965-e591-407b-bb8f-88ef53237031",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(name=\"date\", dataType=StringType()),\n",
    "    StructField(name=\"name_state_ut\", dataType=StringType()),\n",
    "    StructField(name=\"latitude\", dataType=FloatType()),\n",
    "    StructField(name=\"longitude\", dataType=FloatType()),\n",
    "    StructField(name=\"total_confirmed_cases\", dataType=IntegerType()),\n",
    "    StructField(name=\"death\", dataType=IntegerType()),\n",
    "    StructField(name=\"cured_discharged_migrated\", dataType=IntegerType()),\n",
    "    StructField(name=\"new_cases\", dataType=IntegerType()),\n",
    "    StructField(name=\"new_deaths\", dataType=IntegerType()),\n",
    "    StructField(name=\"new_recovered\", dataType=IntegerType())\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"./data.csv\", header=True, schema=schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093615f-ade3-483f-8ae2-1d23c16c35d4",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "In this section, let us clean the data and ensure all columns are in the format we want for processing.\n",
    "\n",
    "### Date Field Cleaning\n",
    "\n",
    "Some date fields are in the format `M/dd/yyyy` and some are in the format `MM-dd-yyyy`. Let's first convert them all to `MM-dd-yyyy` format and convert the field to a date-type field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb78b8-0a59-404d-a2e8-f9d7408dd6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    colName=\"date\",\n",
    "    col=regexp_replace(col(\"date\"), pattern='/', replacement='-')\n",
    ")\n",
    "df = df.withColumn(\n",
    "    colName=\"date\",\n",
    "    col=when(\n",
    "        col(\"date\").rlike(r\"^\\d{1}-\\d{2}-\\d{4}$\"),\n",
    "        date_format(to_date(col(\"date\"), \"M-dd-yyyy\"), \"MM-dd-yyyy\")\n",
    "    ).otherwise(\n",
    "        col(\"date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    colName=\"date\",\n",
    "    col=to_date(col(\"date\"), format=\"MM-dd-yyyy\")\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e322a",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "\n",
    "### Task-1: Convert All State Names to Lowercase\n",
    "\n",
    "To do this, let us transform the `names_state_ut` field's value to lowercase. Note that we're replacing the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3629f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_df = df.withColumn(colName=\"name_state_ut\", col=lower(col(\"name_state_ut\")))\n",
    "\n",
    "task1_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8aac4",
   "metadata": {},
   "source": [
    "### Task-2: Union Teritory with Least Number of Deaths\n",
    "\n",
    "Let us find the Union Territory with least number of deaths with the following steps:\n",
    "\n",
    "1. **Filter:** Let's first filter the data frame by name of the state by choosing only the state names that stats with `Union`.\n",
    "2. **Sum:** Let us next group the result by state name and sum the number of deaths in each grouped states.\n",
    "3. **Sorting and Limiting:** Finally, let's sort the result by `total_deaths` column and limit the result to one which gives the union territory with least death cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "task2_df = df.filter(col(\"name_state_ut\").like(\"%Union%\")) \\\n",
    "    .groupBy(\"name_state_ut\") \\\n",
    "    .sum(\"death\") \\\n",
    "    .withColumnRenamed(\"sum(death)\", \"total_deaths\") \\\n",
    "    .orderBy(\"sum(death)\") \\\n",
    "    .limit(1)\n",
    "\n",
    "task2_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d642f8f7",
   "metadata": {},
   "source": [
    "### Task-3: State with Lowest Death to Total Confirmed Cases Ratio\n",
    "\n",
    "In order to find the state with the lowest death to confirmed cases ratio, let us perform the following steps:\n",
    "\n",
    "1. **Group Data:** Groups the data by state or union territory names.\n",
    "2. **Aggregations:** Calculates the total number of deaths and the total number of confirmed cases for each state or union territory.\n",
    "3. **Calculate Ratio:** Computes the ratio of deaths to confirmed cases for each state or union territory.\n",
    "4. **Sorting and Limiting:** Sorts the data based on this ratio to find the state or union territory with the lowest ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "task3_df = df.groupBy(\"name_state_ut\") \\\n",
    "    .agg(\n",
    "        pyspark_sum(\"death\").alias(\"total_deaths\"),\n",
    "        pyspark_sum(\"total_confirmed_cases\").alias(\"total_total_confirmed_cases\")\n",
    "    ).withColumn(\n",
    "        colName=\"death_confirmed_cases_ratio\",\n",
    "        col=when(\n",
    "            col(\"total_total_confirmed_cases\") != 0,\n",
    "            col(\"total_deaths\") / col(\"total_total_confirmed_cases\")\n",
    "        )\n",
    "    ).orderBy(\"death_confirmed_cases_ratio\").first()\n",
    "\n",
    "print(task3_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d53457",
   "metadata": {},
   "source": [
    "### Task-4: Find the Month with Most Newly Recorded Cases\n",
    "\n",
    "To determine which month experienced the highest number of recoveries, we must go through a few important steps:\n",
    "\n",
    "1. First, we extract the month from each date in the data. This will help us organize the data by month, making it easier to compare the number of recoveries across different time periods.\n",
    "2. Next, we sum-up the number of recoveries for each month. By adding up the daily recovery numbers within each month, we were able to see the total number of recoveries that occurred during that month.\n",
    "3. Finally, we compare the monthly totals to identify which month had the highest number of recoveries. This allows us to pinpoint the specific time period when the most people recovered, providing valuable insights into the recovery trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67018c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "task4_df = df.withColumn(\"month\", month(col(\"date\"))) \\\n",
    "    .groupBy(\"month\") \\\n",
    "    .agg(pyspark_sum(\"new_recovered\").alias(\"total_recovered\")) \\\n",
    "    .orderBy(\"total_recovered\", ascending=False) \\\n",
    "    .first()\n",
    "\n",
    "print(month_name[task4_df[\"month\"]], task4_df, sep=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db29ac7d",
   "metadata": {},
   "source": [
    "### Task-5: The Day with the Most Number of Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task5_df = df.groupBy(\"date\") \\\n",
    "    .agg(pyspark_sum(\"total_confirmed_cases\").alias(\"total_total_confirmed_cases\")) \\\n",
    "    .orderBy(\"total_total_confirmed_cases\", ascending=False) \\\n",
    "    .first()\n",
    "\n",
    "print(task5_df[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1027772",
   "metadata": {},
   "source": [
    "# End Notes\n",
    "\n",
    "The `death` column contains only zero, why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
